"""í•œêµ­ì–´ ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì¢…í•© ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸.

ë‹¤ìŒì„ ê²€ì¦í•©ë‹ˆë‹¤:
- í•œêµ­ì–´ ë¬¸ì„œ ì²­í‚¹ (ë„ì–´ì“°ê¸° ì—†ëŠ” í…ìŠ¤íŠ¸, í•œì˜ í˜¼í•©, ë‹¤ì–‘í•œ í—¤ë”©, í…Œì´ë¸”, ì½”ë“œ ë¸”ë¡)
- í•œêµ­ì–´ íŒŒì¼ëª…/ë””ë ‰í† ë¦¬ëª… ìŠ¤ìºë‹
- í•œêµ­ì–´ íŒŒì´í”„ë¼ì¸ (ì¸ë±ì‹± â†’ ê²€ìƒ‰, ì¦ë¶„ ì¸ë±ì‹±, ê°•ì œ ì¬ì¸ë±ì‹±)
- í•œêµ­ì–´ í•´ì‹œ ë¬´ê²°ì„± (ìœ ë‹ˆì½”ë“œ ì •ê·œí™”, ìëª¨ vs ì™„ì„±í˜•)
- í•œêµ­ì–´ ê²½ê³„ ì¡°ê±´ (50,000ì ë‹¨ë½, ì´ëª¨ì§€ ì¡°í•©, ë‹¨ì¼ ê¸€ì í—¤ë”©)
- ëŒ€ê·œëª¨ í•œêµ­ì–´ ë°ì´í„° í†µí•© (200ê°œ íŒŒì¼, 1000ì¤„ ë¬¸ì„œ)
- í•œêµ­ì–´ CLI ìŠ¤íŠ¸ë ˆìŠ¤ (ì„¤ì •, ë„ì›€ë§, í†µê³„)
"""
from __future__ import annotations

import random
import sys
import unicodedata
from unittest.mock import MagicMock, patch

import pytest

# ---------------------------------------------------------------------------
# zvec ëª¨ë“ˆ ìŠ¤í… (ë„¤ì´í‹°ë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ AVX-512 í•„ìš”)
# ---------------------------------------------------------------------------
if "zvec" not in sys.modules:
    _zvec_stub = MagicMock()
    _zvec_stub.DataType.STRING = "STRING"
    _zvec_stub.DataType.INT32 = "INT32"
    _zvec_stub.DataType.VECTOR_FP32 = "VECTOR_FP32"
    _zvec_stub.DataType.SPARSE_VECTOR_FP32 = "SPARSE_VECTOR_FP32"
    _zvec_stub.MetricType.COSINE = "COSINE"
    _zvec_stub.MetricType.L2 = "L2"
    _zvec_stub.MetricType.IP = "IP"
    _zvec_stub.LogLevel.WARN = "WARN"
    _zvec_stub.FieldSchema = MagicMock
    _zvec_stub.VectorSchema = MagicMock
    _zvec_stub.CollectionSchema = MagicMock
    _zvec_stub.CollectionOption = MagicMock
    _zvec_stub.HnswIndexParam = MagicMock
    _zvec_stub.InvertIndexParam = MagicMock
    _zvec_stub.FlatIndexParam = MagicMock
    _zvec_stub.VectorQuery = MagicMock
    _zvec_stub.RrfReRanker = MagicMock
    _zvec_stub.BM25EmbeddingFunction = MagicMock
    _zvec_stub.Doc = MagicMock
    sys.modules["zvec"] = _zvec_stub

from zvecsearch.chunker import Chunk, chunk_markdown, compute_chunk_id  # noqa: E402
from zvecsearch.core import ZvecSearch  # noqa: E402
from zvecsearch.scanner import scan_paths  # noqa: E402

# ---------------------------------------------------------------------------
# ìƒìˆ˜
# ---------------------------------------------------------------------------
TEST_DIM = 8

# í•œêµ­ì–´ ê¸°ìˆ  ìš©ì–´ í’€
_KOREAN_TOPICS = [
    "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìì—°ì–´ì²˜ë¦¬", "ì»´í“¨í„°ë¹„ì „",
    "ê°•í™”í•™ìŠµ", "ìƒì„±í˜•AI", "íŠ¸ëœìŠ¤í¬ë¨¸", "ì–´í…ì…˜ë©”ì»¤ë‹ˆì¦˜", "ë²¡í„°ê²€ìƒ‰",
    "ì„ë² ë”©", "í† í¬ë‚˜ì´ì €", "íŒŒì¸íŠœë‹", "ì „ì´í•™ìŠµ", "ë°ì´í„°ì¦ê°•",
    "ëª¨ë¸ì••ì¶•", "ì§€ì‹ì¦ë¥˜", "ì—°í•©í•™ìŠµ", "ë©”íƒ€í•™ìŠµ", "ì‹ ê²½ë§ì•„í‚¤í…ì²˜",
    "ê·¸ë˜í”„ì‹ ê²½ë§", "ìˆœí™˜ì‹ ê²½ë§", "í•©ì„±ê³±ì‹ ê²½ë§", "ì˜¤í† ì¸ì½”ë”", "ë³€ë¶„ì¶”ë¡ ",
    "ë² ì´ì§€ì•ˆìµœì í™”", "í•˜ì´í¼íŒŒë¼ë¯¸í„°", "ë°°ì¹˜ì •ê·œí™”", "ë“œë¡­ì•„ì›ƒ", "ì˜µí‹°ë§ˆì´ì €",
    "ê²½ì‚¬í•˜ê°•ë²•", "ì—­ì „íŒŒ", "í™œì„±í™”í•¨ìˆ˜", "ì†ì‹¤í•¨ìˆ˜", "ì •ê·œí™”ê¸°ë²•",
    "êµì°¨ê²€ì¦", "ê³¼ì í•©ë°©ì§€", "ë°ì´í„°ì „ì²˜ë¦¬", "íŠ¹ì„±ê³µí•™", "ì°¨ì›ì¶•ì†Œ",
    "í´ëŸ¬ìŠ¤í„°ë§", "ë¶„ë¥˜ì•Œê³ ë¦¬ì¦˜", "íšŒê·€ë¶„ì„", "ì•™ìƒë¸”ê¸°ë²•", "ë¶€ìŠ¤íŒ…",
    "ëœë¤í¬ë ˆìŠ¤íŠ¸", "ì„œí¬íŠ¸ë²¡í„°ë¨¸ì‹ ", "ê²°ì •íŠ¸ë¦¬", "ë‚˜ì´ë¸Œë² ì´ì¦ˆ", "ë¡œì§€ìŠ¤í‹±íšŒê·€",
]

_KOREAN_SENTENCES = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì€ í˜„ëŒ€ ì‚¬íšŒì˜ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ë³€í™”ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„°ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.",
    "ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” í˜ëª…ì ì¸ ë°œì „ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.",
    "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ìœ ì‚¬ë„ ê²€ìƒ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í•µì‹¬ ì¸í”„ë¼ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì„ íƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.",
    "ì„ë² ë”© ê¸°ìˆ ì„ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ì— ë§¤í•‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "ê°•í™”í•™ìŠµì€ ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ìµœì ì˜ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
    "ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì€ ëª¨ë¸ ì„±ëŠ¥ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œì€ ëŒ€ê·œëª¨ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.",
]


# ---------------------------------------------------------------------------
# ìœ í‹¸ë¦¬í‹°: ìƒíƒœ ê¸°ë°˜ ìŠ¤í† ì–´ Mock
# ---------------------------------------------------------------------------
def _make_stateful_store() -> MagicMock:
    """ìƒíƒœ ê¸°ë°˜ ì¸ë©”ëª¨ë¦¬ ZvecStore Mock."""
    store = MagicMock()
    store._docs: dict[str, dict] = {}

    def _upsert(chunks):
        for c in chunks:
            store._docs[c["chunk_hash"]] = c
        return len(chunks)

    store.upsert.side_effect = _upsert
    store.count.side_effect = lambda: len(store._docs)

    def _hashes_by_source(source):
        return {h for h, c in store._docs.items() if c.get("source") == source}

    store.hashes_by_source.side_effect = _hashes_by_source

    def _existing_hashes(hashes):
        return {h for h in hashes if h in store._docs}

    store.existing_hashes.side_effect = _existing_hashes

    def _delete_by_hashes(hashes):
        for h in hashes:
            store._docs.pop(h, None)

    store.delete_by_hashes.side_effect = _delete_by_hashes

    def _delete_by_source(source):
        to_del = [h for h, c in store._docs.items() if c.get("source") == source]
        for h in to_del:
            del store._docs[h]

    store.delete_by_source.side_effect = _delete_by_source

    def _search(query_embedding, query_text="", top_k=10, filter_expr=""):
        results = list(store._docs.values())[:top_k]
        return [
            {
                "content": c.get("content", ""),
                "source": c.get("source", ""),
                "heading": c.get("heading", ""),
                "heading_level": c.get("heading_level", 0),
                "start_line": c.get("start_line", 0),
                "end_line": c.get("end_line", 0),
                "chunk_hash": c.get("chunk_hash", ""),
                "score": 0.95 - i * 0.01,
            }
            for i, c in enumerate(results)
        ]

    store.search.side_effect = _search
    store.close.return_value = None
    store.drop.return_value = None
    return store


class FakeEmbedder:
    """Mock ì˜¤ì—¼ì„ í”¼í•˜ê¸° ìœ„í•œ ìˆœìˆ˜ Python ì„ë² ë”© ì œê³µì."""

    def __init__(self, dim: int = TEST_DIM):
        self.model_name = "korean-stress-test-model"
        self.dimension = dim
        self._dim = dim
        self._embed_calls: list[list[str]] = []
        self._custom_return = None

    async def embed(self, texts: list[str]) -> list[list[float]]:
        self._embed_calls.append(texts)
        if self._custom_return is not None:
            return self._custom_return
        return [[float(i + 1) * 0.1] * self._dim for i, _ in enumerate(texts)]


def _make_embedder(dim: int = TEST_DIM) -> FakeEmbedder:
    return FakeEmbedder(dim=dim)


# ---------------------------------------------------------------------------
# í•œêµ­ì–´ ëŒ€ê·œëª¨ ë°ì´í„° ìƒì„± ìœ í‹¸ë¦¬í‹°
# ---------------------------------------------------------------------------
def _generate_korean_article(
    num_sections: int = 20,
    paragraphs_per: int = 3,
) -> str:
    """í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ ìƒì„± (AI/ML ì£¼ì œ)."""
    parts = []
    for i in range(num_sections):
        level = (i % 4) + 1
        topic = _KOREAN_TOPICS[i % len(_KOREAN_TOPICS)]
        parts.append(f"{'#' * level} {topic} ê¸°ìˆ  ê°œìš” â€” ì„¹ì…˜ {i + 1}")
        for p in range(paragraphs_per):
            sentence = _KOREAN_SENTENCES[(i + p) % len(_KOREAN_SENTENCES)]
            parts.append(f"\n{sentence} {topic}ì— ëŒ€í•œ {p + 1}ë²ˆì§¸ ì„¤ëª…ì…ë‹ˆë‹¤. "
                         f"ì´ ë¶„ì•¼ëŠ” ìµœê·¼ ê¸‰ê²©í•œ ë°œì „ì„ ì´ë£¨ê³  ìˆìœ¼ë©°, "
                         f"ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n")
    return "\n".join(parts)


def _generate_korean_mixed_markdown() -> str:
    """í•œì˜ í˜¼í•© ë§ˆí¬ë‹¤ìš´ (ì½”ë“œ ë¸”ë¡, ê¸°ìˆ  ìš©ì–´)."""
    return """# ZvecSearch ì•„í‚¤í…ì²˜ ì„¤ê³„

ì´ ë¬¸ì„œëŠ” ZvecSearchì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

## Backend Architecture

ZvecSearchëŠ” `zvec` ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
HNSW(Hierarchical Navigable Small World) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬
ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ(ANN) ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ì½”ë“œ êµ¬ì¡°

```python
class ZvecSearch:
    \"\"\"í•µì‹¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í´ë˜ìŠ¤.\"\"\"

    async def index(self, force: bool = False) -> int:
        # íŒŒì¼ ìŠ¤ìº” â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥
        files = scan_paths(self._paths)
        for f in files:
            await self._index_file(f, force=force)

    async def search(self, query: str, top_k: int = 10):
        # ì¿¼ë¦¬ ì„ë² ë”© â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        embeddings = await self._embedder.embed([query])
        return self._store.search(embeddings[0], query)
```

## ì„ë² ë”© ì‹œìŠ¤í…œ (Embedding System)

5ê°œì˜ embedding providerë¥¼ ì§€ì›í•©ë‹ˆë‹¤:
- **OpenAI**: `text-embedding-3-small` (ê¸°ë³¸ê°’, 1536ì°¨ì›)
- **Google**: `text-embedding-004` (768ì°¨ì›)
- **Voyage AI**: `voyage-3-lite` (1024ì°¨ì›)
- **Ollama**: ë¡œì»¬ ëª¨ë¸ ì§€ì› (ê°€ë³€ ì°¨ì›)
- **Local**: `sentence-transformers` ê¸°ë°˜ (384ì°¨ì›)

### í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Hybrid Search)

Dense vector (HNSW/cosine) + Sparse vector (BM25)ë¥¼ ê²°í•©í•˜ì—¬
RRF (Reciprocal Rank Fusion, k=60)ë¡œ ì¬ë­í‚¹í•©ë‹ˆë‹¤.

## ì„±ëŠ¥ ìµœì í™”

### ì¦ë¶„ ì¸ë±ì‹± (Incremental Indexing)

íŒŒì¼ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ re-embeddingì„ ê±´ë„ˆëœë‹ˆë‹¤.
`chunk_hash`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½ ê°ì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ë©”ëª¨ë¦¬ ìµœì í™”

zvecì˜ `enable_mmap=True` ì„¤ì •ìœ¼ë¡œ memory-mapped I/Oë¥¼ í™œìš©í•©ë‹ˆë‹¤.
"""


def _generate_korean_table_heavy() -> str:
    """í•œêµ­ì–´ í…Œì´ë¸” ìœ„ì£¼ ë¬¸ì„œ."""
    parts = ["# í•œêµ­ì–´ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ\n"]
    parts.append("## ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ\n")
    parts.append("| ëª¨ë¸ëª… | ì •í™•ë„ | F1 ì ìˆ˜ | ì¶”ë¡  ì‹œê°„(ms) | ë©”ëª¨ë¦¬(GB) |")
    parts.append("|--------|--------|---------|---------------|------------|")
    models = ["BERT-ko", "KoGPT-2", "KoBART", "KoELECTRA", "KoBigBird",
              "KoT5", "KoALBERT", "HyperCLOVA", "KoRoBERTa", "KoXLNet",
              "Polyglot-Ko", "LLaMA-Ko", "Gemma-Ko", "Solar", "EXAONE"]
    for m in models:
        acc = f"{random.uniform(85, 99):.1f}%"
        f1 = f"{random.uniform(80, 98):.2f}"
        latency = f"{random.randint(5, 200)}"
        mem = f"{random.uniform(0.5, 16):.1f}"
        parts.append(f"| {m} | {acc} | {f1} | {latency} | {mem} |")

    parts.append("\n## ë°ì´í„°ì…‹ í†µê³„\n")
    parts.append("| ë°ì´í„°ì…‹ | ë¬¸ì„œ ìˆ˜ | í‰ê·  ê¸¸ì´ | ì–¸ì–´ | ë„ë©”ì¸ |")
    parts.append("|----------|---------|----------|------|--------|")
    datasets = [
        ("KorQuAD 2.0", "10ë§Œ", "350ì", "í•œêµ­ì–´", "ìœ„í‚¤ë°±ê³¼"),
        ("KLUE-NLI", "5ë§Œ", "120ì", "í•œêµ­ì–´", "ë‰´ìŠ¤"),
        ("í•œêµ­ì–´ ìœ„í‚¤", "80ë§Œ", "2000ì", "í•œêµ­ì–´", "ë°±ê³¼ì‚¬ì „"),
        ("ë„¤ì´ë²„ ì˜í™”ë¦¬ë·°", "20ë§Œ", "80ì", "í•œêµ­ì–´", "ë¦¬ë·°"),
        ("êµ­ë¦½êµ­ì–´ì› ë§ë­‰ì¹˜", "100ë§Œ", "500ì", "í•œêµ­ì–´", "ì¼ë°˜"),
        ("AI Hub ëŒ€í™”", "30ë§Œ", "60ì", "í•œêµ­ì–´", "ëŒ€í™”"),
        ("ë²•ë¥  íŒë¡€", "50ë§Œ", "3000ì", "í•œêµ­ì–´", "ë²•ë¥ "),
        ("ì˜í•™ ë…¼ë¬¸", "15ë§Œ", "5000ì", "í•œêµ­ì–´/ì˜ì–´", "ì˜í•™"),
        ("íŠ¹í—ˆ ë¬¸ì„œ", "200ë§Œ", "1500ì", "í•œêµ­ì–´", "íŠ¹í—ˆ"),
        ("ì†Œì…œë¯¸ë””ì–´", "500ë§Œ", "40ì", "í•œêµ­ì–´", "ì†Œì…œ"),
    ]
    for name, docs, avg_len, lang, domain in datasets:
        parts.append(f"| {name} | {docs} | {avg_len} | {lang} | {domain} |")

    return "\n".join(parts)


def _generate_korean_no_spaces(length: int = 2000) -> str:
    """ë„ì–´ì“°ê¸° ì—†ëŠ” ê¸´ í•œêµ­ì–´ í…ìŠ¤íŠ¸."""
    base = "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜"
    # ìëª¨ ì¡°í•©ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸€ì ìƒì„±
    chars = []
    for i in range(length):
        idx = (i * 7 + 3) % len(base)
        chars.append(base[idx])
    return "".join(chars)


def _generate_korean_files(tmp_path, count: int = 100) -> list:
    """ëŒ€ëŸ‰ í•œêµ­ì–´ íŒŒì¼ ìƒì„±."""
    files = []
    for i in range(count):
        topic = _KOREAN_TOPICS[i % len(_KOREAN_TOPICS)]
        content = f"# {topic} ë¬¸ì„œ {i + 1}\n\n"
        content += f"{_KOREAN_SENTENCES[i % len(_KOREAN_SENTENCES)]}\n\n"
        content += f"ì´ ë¬¸ì„œëŠ” {topic}ì— ëŒ€í•œ {i + 1}ë²ˆì§¸ ìë£Œì…ë‹ˆë‹¤. "
        content += "ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ê¸°ìˆ ì˜ ë°œì „ ë°©í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n"
        fname = f"ë¬¸ì„œ_{i:03d}_{topic}.md"
        path = tmp_path / fname
        path.write_text(content, encoding="utf-8")
        files.append(path)
    return files


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 1: í•œêµ­ì–´ ì²­í‚¹ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestKoreanChunkerStress:
    """í•œêµ­ì–´ ë¬¸ì„œì— ëŒ€í•œ ì²­í‚¹ ì •í™•ì„± í…ŒìŠ¤íŠ¸."""

    def test_korean_long_article_chunking(self):
        """50ê°œ ì„¹ì…˜ í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ ì²­í‚¹."""
        md = _generate_korean_article(num_sections=50, paragraphs_per=3)
        chunks = chunk_markdown(md, source="í•œêµ­ì–´ê¸°ìˆ ë¬¸ì„œ.md", max_chunk_size=800)
        assert len(chunks) >= 50
        # ëª¨ë“  ì²­í¬ì— í•œêµ­ì–´ í¬í•¨
        for c in chunks:
            has_korean = any("\uAC00" <= ch <= "\uD7A3" for ch in c.content)
            assert has_korean, f"í•œêµ­ì–´ê°€ ì—†ëŠ” ì²­í¬: {c.content[:50]}"

    def test_korean_no_space_text_splitting(self):
        """ë„ì–´ì“°ê¸° ì—†ëŠ” 2000ì í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„í• ."""
        no_space = _generate_korean_no_spaces(2000)
        md = f"# ë„ì–´ì“°ê¸°ì—†ëŠ”ë¬¸ì¥\n\n{no_space}"
        chunks = chunk_markdown(md, source="nospace.md", max_chunk_size=500)
        assert len(chunks) >= 1
        total_korean = sum(
            sum(1 for ch in c.content if "\uAC00" <= ch <= "\uD7A3")
            for c in chunks
        )
        assert total_korean >= 2000

    def test_korean_mixed_hangul_english_chunking(self):
        """í•œì˜ í˜¼í•© ë¬¸ì„œ ì²­í‚¹."""
        md = _generate_korean_mixed_markdown()
        chunks = chunk_markdown(md, source="mixed.md", max_chunk_size=600)
        assert len(chunks) >= 5
        # í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ë³´ì¡´
        all_text = " ".join(c.content for c in chunks)
        assert "ì•„í‚¤í…ì²˜" in all_text
        assert "ZvecSearch" in all_text
        assert "embedding" in all_text.lower()
        assert "í•˜ì´ë¸Œë¦¬ë“œ" in all_text

    def test_korean_heading_variations(self):
        """ë‹¤ì–‘í•œ í•œêµ­ì–´ í—¤ë”© í˜•ì‹ íŒŒì‹±."""
        md = """# ê¸°ë³¸ í•œêµ­ì–´ í—¤ë”©

ë‚´ìš©ì…ë‹ˆë‹¤.

## íŠ¹ìˆ˜ë¬¸ì í¬í•¨: ë²¡í„°(Vector) ê²€ìƒ‰!

ì„¤ëª…ì…ë‹ˆë‹¤.

### ìˆ«ì í¬í•¨ â€” 3ê°€ì§€ ë°©ë²•ë¡ 

ë°©ë²•ë¡  ì„¤ëª….

#### ê´„í˜¸ì™€ ê¸°í˜¸ [ì°¸ê³ ] â˜…ì¤‘ìš”â˜…

ì¤‘ìš”í•œ ë‚´ìš©.

##### ê¸´ í—¤ë”©: ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ìì—°ì–´ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ ì„¤ê³„ì™€ êµ¬í˜„ì— ê´€í•œ ì—°êµ¬

ì—°êµ¬ ë‚´ìš©.

###### ë§ˆì§€ë§‰ ë ˆë²¨ â€” ìš”ì•½ ë° ê²°ë¡ 

ê²°ë¡ ì…ë‹ˆë‹¤.
"""
        chunks = chunk_markdown(md, source="headings.md")
        headings = [c.heading for c in chunks if c.heading]
        assert "ê¸°ë³¸ í•œêµ­ì–´ í—¤ë”©" in headings
        assert any("ë²¡í„°" in h for h in headings)
        assert any("3ê°€ì§€" in h for h in headings)
        assert any("â˜…ì¤‘ìš”â˜…" in h for h in headings)
        levels = {c.heading_level for c in chunks if c.heading_level > 0}
        assert levels == {1, 2, 3, 4, 5, 6}

    def test_korean_table_chunking(self):
        """í•œêµ­ì–´ í…Œì´ë¸” ë¬¸ì„œ ì²­í‚¹."""
        md = _generate_korean_table_heavy()
        chunks = chunk_markdown(md, source="tables.md", max_chunk_size=1500)
        assert len(chunks) >= 2
        all_text = " ".join(c.content for c in chunks)
        assert "BERT-ko" in all_text
        assert "KorQuAD" in all_text
        # í…Œì´ë¸” êµ¬ë¶„ì ë³´ì¡´
        assert "|" in all_text

    def test_korean_code_block_with_comments(self):
        """í•œêµ­ì–´ ì£¼ì„ì´ ìˆëŠ” ì½”ë“œ ë¸”ë¡ ë³´ì¡´."""
        md = """# ì½”ë“œ ì˜ˆì œ

```python
# í•œêµ­ì–´ ì£¼ì„: ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def ì „ì²˜ë¦¬(í…ìŠ¤íŠ¸: str) -> str:
    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\"\"\"
    ê²°ê³¼ = í…ìŠ¤íŠ¸.strip()
    return ê²°ê³¼

# ì‹¤í–‰ ì˜ˆì‹œ
print(ì „ì²˜ë¦¬("  ì•ˆë…•í•˜ì„¸ìš”  "))
```

## ì‹¤í–‰ ê²°ê³¼

ìœ„ ì½”ë“œì˜ ì‹¤í–‰ ê²°ê³¼ëŠ” "ì•ˆë…•í•˜ì„¸ìš”"ì…ë‹ˆë‹¤.
"""
        chunks = chunk_markdown(md, source="code.md")
        all_text = " ".join(c.content for c in chunks)
        assert "ì „ì²˜ë¦¬" in all_text
        assert "í•œêµ­ì–´ ì£¼ì„" in all_text

    def test_korean_blockquote_and_list(self):
        """í•œêµ­ì–´ ì¸ìš©ë¬¸, ëª©ë¡ ë³´ì¡´."""
        md = """# í•œêµ­ì–´ ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°

> "ì§€ì‹ì€ í˜ì´ë‹¤." - í”„ëœì‹œìŠ¤ ë² ì´ì»¨
> ì´ ì¸ìš©ë¬¸ì€ ì§€ì‹ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ìˆœì„œ ì—†ëŠ” ëª©ë¡

- ì²« ë²ˆì§¸ í•­ëª©: ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬
- ë‘ ë²ˆì§¸ í•­ëª©: í˜„ì¬ ë™í–¥
  - í•˜ìœ„ í•­ëª©: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸
  - í•˜ìœ„ í•­ëª©: ë©€í‹°ëª¨ë‹¬ AI
- ì„¸ ë²ˆì§¸ í•­ëª©: ë¯¸ë˜ ì „ë§

## ìˆœì„œ ìˆëŠ” ëª©ë¡

1. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„
2. ëª¨ë¸ í•™ìŠµ ë‹¨ê³„
3. í‰ê°€ ë° ë°°í¬ ë‹¨ê³„
"""
        chunks = chunk_markdown(md, source="lists.md")
        all_text = " ".join(c.content for c in chunks)
        assert "í”„ëœì‹œìŠ¤ ë² ì´ì»¨" in all_text
        assert "ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬" in all_text
        assert "ë©€í‹°ëª¨ë‹¬ AI" in all_text

    def test_korean_special_characters(self):
        """í•œêµ­ì–´ íŠ¹ìˆ˜ ë¶€í˜¸, ìëª¨ ë³´ì¡´."""
        md = """# íŠ¹ìˆ˜ ë¬¸ì í…ŒìŠ¤íŠ¸

## í•œê¸€ ìëª¨

ã„± ã„´ ã„· ã„¹ ã… ã…‚ ã…… ã…‡ ã…ˆ ã…Š ã…‹ ã…Œ ã… ã…
ã… ã…‘ ã…“ ã…• ã…— ã…› ã…œ ã…  ã…¡ ã…£

## íŠ¹ìˆ˜ ë¶€í˜¸

â€» ì£¼ì˜ì‚¬í•­: ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”.
â˜… ì¤‘ìš”: í•µì‹¬ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.
â—† ì°¸ê³ : ì¶”ê°€ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤.
â–  ê²°ë¡ : ìµœì¢… ìš”ì•½ì…ë‹ˆë‹¤.
â— í•­ëª©: ì„¸ë¶€ ë‚´ìš©ì…ë‹ˆë‹¤.
â†’ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”.
ã€Œì¸ìš©ã€ ã€ì°¸ê³ ë¬¸í—Œã€ ã€ë³´ì¶© ì„¤ëª…ã€‘

## í•œêµ­ì–´ ê´„í˜¸ í‘œí˜„

(ê°€) ì²« ë²ˆì§¸ ì¡°ê±´
(ë‚˜) ë‘ ë²ˆì§¸ ì¡°ê±´
(ë‹¤) ì„¸ ë²ˆì§¸ ì¡°ê±´
"""
        chunks = chunk_markdown(md, source="special.md")
        all_text = " ".join(c.content for c in chunks)
        assert "ã„±" in all_text
        assert "ã…" in all_text
        assert "â€»" in all_text
        assert "â˜…" in all_text
        assert "ã€Œì¸ìš©ã€" in all_text
        assert "(ê°€)" in all_text


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 2: í•œêµ­ì–´ ìŠ¤ìºë„ˆ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestKoreanScannerStress:
    """í•œêµ­ì–´ íŒŒì¼ëª…/ë””ë ‰í† ë¦¬ëª… ìŠ¤ìºë‹ í…ŒìŠ¤íŠ¸."""

    def test_scan_100_korean_named_files(self, tmp_path):
        """í•œêµ­ì–´ íŒŒì¼ëª… 100ê°œ ìŠ¤ìºë‹."""
        _generate_korean_files(tmp_path, count=100)
        results = scan_paths([tmp_path])
        assert len(results) == 100

    def test_korean_directory_names(self, tmp_path):
        """í•œêµ­ì–´ ë””ë ‰í† ë¦¬ëª… 5ë‹¨ê³„ ì¤‘ì²© ìŠ¤ìºë‹."""
        dirs = ["í”„ë¡œì íŠ¸", "ë¬¸ì„œê´€ë¦¬", "ê¸°ìˆ ìë£Œ", "ì¸ê³µì§€ëŠ¥", "ë²¡í„°ê²€ìƒ‰"]
        current = tmp_path
        for d in dirs:
            current = current / d
            current.mkdir()
            (current / f"{d}_ì„¤ëª….md").write_text(f"# {d}\n\n{d}ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.")
        results = scan_paths([tmp_path])
        assert len(results) == 5

    def test_mixed_korean_english_filenames(self, tmp_path):
        """í•œì˜ í˜¼í•© íŒŒì¼ëª… ìŠ¤ìºë‹."""
        names = [
            "AIê¸°ìˆ _overview.md",
            "ë¨¸ì‹ ëŸ¬ë‹_tutorial_v2.md",
            "deep_learning_ë”¥ëŸ¬ë‹.md",
            "NLP_ìì—°ì–´ì²˜ë¦¬_guide.md",
            "vector_ë²¡í„°_search.md",
        ]
        for name in names:
            (tmp_path / name).write_text(f"# {name}\në‚´ìš©")
        results = scan_paths([tmp_path])
        assert len(results) == 5

    def test_korean_filenames_with_spaces(self, tmp_path):
        """ê³µë°± í¬í•¨ í•œêµ­ì–´ íŒŒì¼ëª… ì²˜ë¦¬."""
        names = [
            "ì¸ê³µì§€ëŠ¥ ê°œë¡ .md",
            "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ê°•ì˜.md",
            "ë”¥ëŸ¬ë‹ ì‹¤ìŠµ ìë£Œ ëª¨ìŒ.md",
        ]
        for name in names:
            (tmp_path / name).write_text(f"# {name}\në‚´ìš©")
        results = scan_paths([tmp_path])
        assert len(results) == 3

    def test_korean_filename_sorting(self, tmp_path):
        """í•œêµ­ì–´ íŒŒì¼ëª… ì •ë ¬ ìˆœì„œ ì¼ê´€ì„±."""
        names = ["ë‹¤.md", "ê°€.md", "ë‚˜.md", "ë¼.md", "ë§ˆ.md"]
        for name in names:
            (tmp_path / name).write_text(f"# {name}\në‚´ìš©")
        results = scan_paths([tmp_path])
        result_names = [r.path.name for r in results]
        assert result_names == sorted(result_names)


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 3: í•œêµ­ì–´ íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestKoreanPipelineStress:
    """í•œêµ­ì–´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸."""

    @pytest.mark.asyncio
    async def test_index_100_korean_files(self, tmp_path):
        """100ê°œ í•œêµ­ì–´ íŒŒì¼ ì¸ë±ì‹±."""
        _generate_korean_files(tmp_path, count=100)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count = await zs.index()
            assert count == 100
            assert store.count() == 100
            # í•œêµ­ì–´ ë‚´ìš© ë³´ì¡´ í™•ì¸
            stored = [d["content"] for d in store._docs.values()]
            korean_count = sum(
                1 for s in stored
                if any("\uAC00" <= ch <= "\uD7A3" for ch in s)
            )
            assert korean_count == 100
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_search_returns_korean_content(self, tmp_path):
        """í•œêµ­ì–´ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰."""
        for i in range(10):
            topic = _KOREAN_TOPICS[i]
            (tmp_path / f"doc_{i}.md").write_text(
                f"# {topic}\n\n{topic}ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…ì…ë‹ˆë‹¤."
            )

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            await zs.index()

            emb._custom_return = [[0.5] * TEST_DIM]
            results = await zs.search("ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ", top_k=5)
            assert len(results) <= 5
            assert all("content" in r for r in results)
            # ê²°ê³¼ì— í•œêµ­ì–´ í¬í•¨
            for r in results:
                has_korean = any("\uAC00" <= ch <= "\uD7A3" for ch in r["content"])
                assert has_korean
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_incremental_add(self, tmp_path):
        """í•œêµ­ì–´ íŒŒì¼ 50ê°œ ì¸ë±ì‹± í›„ 20ê°œ ì¶”ê°€."""
        _generate_korean_files(tmp_path, count=50)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count1 = await zs.index()
            assert count1 == 50

            # 20ê°œ ì¶”ê°€
            for i in range(50, 70):
                topic = _KOREAN_TOPICS[i % len(_KOREAN_TOPICS)]
                (tmp_path / f"ì¶”ê°€ë¬¸ì„œ_{i}.md").write_text(
                    f"# {topic} ì¶”ê°€\n\nì¶”ê°€ëœ ë¬¸ì„œì…ë‹ˆë‹¤."
                )

            count2 = await zs.index()
            assert count2 == 20
            assert store.count() == 70
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_incremental_modify(self, tmp_path):
        """í•œêµ­ì–´ íŒŒì¼ ë‚´ìš© ë³€ê²½ í›„ ì¦ë¶„ ì¸ë±ì‹±."""
        fpath = tmp_path / "ë³€ê²½ëŒ€ìƒ.md"
        fpath.write_text("# ì›ë³¸ ë¬¸ì„œ\n\nì›ë³¸ ë‚´ìš©ì…ë‹ˆë‹¤.")

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            await zs.index()
            assert store.count() == 1

            # ë‚´ìš© ë³€ê²½
            fpath.write_text("# ìˆ˜ì •ëœ ë¬¸ì„œ\n\nìˆ˜ì •ëœ ë‚´ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            count2 = await zs.index()
            assert count2 == 1  # ìƒˆ ì²­í¬
            assert store.count() == 1  # ì´ì „ ê²ƒ ì‚­ì œ, ìƒˆ ê²ƒ ì¶”ê°€
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_incremental_delete_and_reindex(self, tmp_path):
        """íŒŒì¼ ì‚­ì œ í›„ ì¬ì¸ë±ì‹±."""
        for i in range(5):
            (tmp_path / f"ì‚­ì œí…ŒìŠ¤íŠ¸_{i}.md").write_text(
                f"# ë¬¸ì„œ {i}\n\në‚´ìš© {i}ì…ë‹ˆë‹¤."
            )

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            await zs.index()
            assert store.count() == 5

            # 2ê°œ íŒŒì¼ ì‚­ì œ
            (tmp_path / "ì‚­ì œí…ŒìŠ¤íŠ¸_0.md").unlink()
            (tmp_path / "ì‚­ì œí…ŒìŠ¤íŠ¸_1.md").unlink()

            # ì¬ì¸ë±ì‹± â€” ë‚¨ì€ 3ê°œë§Œ ì¬ì¸ë±ì‹± (ì‚­ì œëœ íŒŒì¼ì€ scanì—ì„œ ë¹ ì§)
            await zs.index()
            # ì‚­ì œëœ íŒŒì¼ì˜ ì†ŒìŠ¤ê°€ storeì—ì„œ ìë™ ì œê±°ë˜ì§€ ì•ŠìŒ (scanì— ì—†ìœ¼ë¯€ë¡œ)
            # í•˜ì§€ë§Œ ë‚¨ì€ íŒŒì¼ì€ ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ 0ê°œ ìƒˆ ì²­í¬
            # ì´ storeì—ëŠ” ì—¬ì „íˆ 5ê°œ (ì‚­ì œëœ ì†ŒìŠ¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ìš°ì§€ ì•Šìœ¼ë©´)
            assert store.count() >= 3
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_force_reindex_large(self, tmp_path):
        """50ê°œ í•œêµ­ì–´ íŒŒì¼ ê°•ì œ ì¬ì¸ë±ì‹±."""
        _generate_korean_files(tmp_path, count=50)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count1 = await zs.index()
            assert count1 == 50

            count2 = await zs.index(force=True)
            assert count2 == 50
            assert store.count() == 50
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_multi_search_cycle(self, tmp_path):
        """10íšŒ ì¸ë±ì‹±-ê²€ìƒ‰ ë°˜ë³µ ì‚¬ì´í´."""
        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")

            for cycle in range(10):
                topic = _KOREAN_TOPICS[cycle]
                (tmp_path / f"ì‚¬ì´í´_{cycle}_{topic}.md").write_text(
                    f"# {topic}\n\n{topic}ì— ëŒ€í•œ ì‚¬ì´í´ {cycle} ë¬¸ì„œ."
                )
                count = await zs.index()
                assert count == 1

                emb._custom_return = [[0.5] * TEST_DIM]
                results = await zs.search(f"{topic} ê²€ìƒ‰", top_k=50)
                assert len(results) == cycle + 1

            assert store.count() == 10
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_mixed_language_pipeline(self, tmp_path):
        """í•œêµ­ì–´+ì˜ì–´+ì¼ë³¸ì–´ í˜¼í•© íŒŒì¼ íŒŒì´í”„ë¼ì¸."""
        (tmp_path / "korean.md").write_text(
            "# í•œêµ­ì–´ ë¬¸ì„œ\n\nì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì— ëŒ€í•œ í•œêµ­ì–´ ì„¤ëª…ì…ë‹ˆë‹¤."
        )
        (tmp_path / "english.md").write_text(
            "# English Document\n\nThis is about artificial intelligence."
        )
        (tmp_path / "japanese.md").write_text(
            "# æ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n\näººå·¥çŸ¥èƒ½æŠ€è¡“ã«ã¤ã„ã¦ã®èª¬æ˜ã§ã™ã€‚"
        )
        (tmp_path / "mixed.md").write_text(
            "# í˜¼í•© Mixed æ··åˆ\n\ní•œêµ­ì–´ English æ—¥æœ¬èª í•¨ê»˜."
        )

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count = await zs.index()
            assert count == 4
            stored = {d["source"].split("/")[-1] for d in store._docs.values()}
            assert "korean.md" in stored
            assert "english.md" in stored
            assert "japanese.md" in stored
            assert "mixed.md" in stored
            zs.close()


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 4: í•œêµ­ì–´ í•´ì‹œ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestKoreanHashIntegrity:
    """í•œêµ­ì–´ ì½˜í…ì¸ ì˜ í•´ì‹œ ë° ID ë¬´ê²°ì„± ê²€ì¦."""

    def test_korean_content_hash_uniqueness(self):
        """500ê°œ ê³ ìœ  í•œêµ­ì–´ ì½˜í…ì¸  â†’ 500ê°œ ê³ ìœ  í•´ì‹œ."""
        hashes = set()
        for i in range(500):
            topic = _KOREAN_TOPICS[i % len(_KOREAN_TOPICS)]
            content = f"{topic}ì— ëŒ€í•œ ë¬¸ì„œ ë²ˆí˜¸ {i}ì…ë‹ˆë‹¤. ê³ ìœ í•œ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤."
            c = Chunk(content=content, source="hash.md", heading="",
                      heading_level=0, start_line=1, end_line=1)
            hashes.add(c.content_hash)
        assert len(hashes) == 500

    def test_korean_similar_content_different_hashes(self):
        """ë¹„ìŠ·í•œ í•œêµ­ì–´ ë¬¸ì¥(ì¡°ì‚¬ë§Œ ë‹¤ë¥¸) â†’ ë‹¤ë¥¸ í•´ì‹œ."""
        variations = [
            "ì¸ê³µì§€ëŠ¥ì´ ë°œì „í•˜ê³  ìˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ì€ ë°œì „í•˜ê³  ìˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ì„ ë°œì „ì‹œí‚¤ê³  ìˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì´ ê³„ì†ë˜ê³  ìˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ì— ëŒ€í•œ ë°œì „ì´ ì´ë£¨ì–´ì§€ê³  ìˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ì¸í•œ ë°œì „ì´ ëˆˆì— ëˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ê³¼ í•¨ê»˜ ë°œì „í•˜ê³  ìˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ì—ì„œ ë°œì „ì´ ì¼ì–´ë‚˜ê³  ìˆë‹¤.",
        ]
        hashes = set()
        for v in variations:
            c = Chunk(content=v, source="josa.md", heading="",
                      heading_level=0, start_line=1, end_line=1)
            hashes.add(c.content_hash)
        assert len(hashes) == len(variations)

    def test_korean_chunk_id_deterministic(self):
        """ë™ì¼ í•œêµ­ì–´ ì…ë ¥ 100íšŒ ë°˜ë³µ â†’ í•­ìƒ ê°™ì€ ID."""
        for _ in range(100):
            cid = compute_chunk_id("í•œêµ­ì–´ë¬¸ì„œ.md", 1, 10, "í•´ì‹œê°’abc", "ëª¨ë¸ëª…")
            assert cid == compute_chunk_id("í•œêµ­ì–´ë¬¸ì„œ.md", 1, 10, "í•´ì‹œê°’abc", "ëª¨ë¸ëª…")

    def test_korean_unicode_normalization_hashing(self):
        """NFC vs NFD ì •ê·œí™” í˜•íƒœ â†’ í•´ì‹œ ì°¨ì´ ê²€ì¦."""
        # "í•œ" in NFC (composed) vs NFD (decomposed)
        nfc_text = unicodedata.normalize("NFC", "í•œêµ­ì–´ í…ŒìŠ¤íŠ¸")
        nfd_text = unicodedata.normalize("NFD", "í•œêµ­ì–´ í…ŒìŠ¤íŠ¸")
        # NFCì™€ NFDëŠ” ë°”ì´íŠ¸ í‘œí˜„ì´ ë‹¤ë¥´ë¯€ë¡œ í•´ì‹œë„ ë‹¬ë¼ì•¼ í•¨
        c1 = Chunk(content=nfc_text, source="a.md", heading="",
                   heading_level=0, start_line=1, end_line=1)
        c2 = Chunk(content=nfd_text, source="a.md", heading="",
                   heading_level=0, start_line=1, end_line=1)
        if nfc_text.encode("utf-8") != nfd_text.encode("utf-8"):
            assert c1.content_hash != c2.content_hash
        else:
            assert c1.content_hash == c2.content_hash

    def test_korean_jamo_vs_syllable_hashing(self):
        """ìëª¨ ì¡°í•© vs ì™„ì„±í˜• ìŒì ˆ â†’ ë‹¤ë¥¸ í•´ì‹œ."""
        # ã…ã…ã„´ (3 jamo) vs í•œ (1 syllable)
        jamo = "ã…ã…ã„´"
        syllable = "í•œ"
        c1 = Chunk(content=jamo, source="a.md", heading="",
                   heading_level=0, start_line=1, end_line=1)
        c2 = Chunk(content=syllable, source="a.md", heading="",
                   heading_level=0, start_line=1, end_line=1)
        assert c1.content_hash != c2.content_hash


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 5: í•œêµ­ì–´ ê²½ê³„ ì¡°ê±´ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestKoreanBoundaryConditions:
    """í•œêµ­ì–´ íŠ¹í™” ê²½ê³„ ì¡°ê±´ í…ŒìŠ¤íŠ¸."""

    def test_only_korean_punctuation(self):
        """í•œêµ­ì–´ ë¬¸ì¥ë¶€í˜¸ë§Œ ìˆëŠ” ì„¹ì…˜."""
        md = "# ì œëª©\n\nã€‚ã€ã€Œã€ã€ã€ã€ã€‘\n\n## ë‹¤ìŒ ì„¹ì…˜\n\në‚´ìš©"
        chunks = chunk_markdown(md, source="punct.md")
        assert len(chunks) >= 2
        # ë§ˆì§€ë§‰ ì„¹ì…˜ì€ "ë‚´ìš©"ì„ í¬í•¨
        assert any("ë‚´ìš©" in c.content for c in chunks)

    def test_korean_emoji_combination(self):
        """í•œêµ­ì–´ + ì´ëª¨ì§€ í˜¼í•© í…ìŠ¤íŠ¸ ë³´ì¡´."""
        md = """# ê°ì • ë¶„ì„ ğŸ¤–

ì´ ë¬¸ì„œëŠ” ê°ì • ë¶„ì„ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤ ğŸ˜Š

## ê¸ì • ğŸ˜ƒ vs ë¶€ì • ğŸ˜¢

ê¸ì •ì  ë¦¬ë·°: "ì´ ì œí’ˆì€ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤! ğŸ‘ğŸ‰"
ë¶€ì •ì  ë¦¬ë·°: "ì‹¤ë§ìŠ¤ëŸ½ìŠµë‹ˆë‹¤ ğŸ˜ğŸ’”"

### ì´ëª¨ì§€ í™œìš© ì‚¬ë¡€ ğŸŒŸ

- ì±—ë´‡ ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”! ğŸ™‹â€â™‚ï¸
- ì•Œë¦¼: ì‘ì—… ì™„ë£Œ! âœ…
- ê²½ê³ : ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤! âš ï¸ğŸ”´
"""
        chunks = chunk_markdown(md, source="emoji.md")
        all_text = " ".join(c.content for c in chunks)
        assert "ğŸ¤–" in all_text
        assert "ğŸ˜Š" in all_text
        assert "ê°ì • ë¶„ì„" in all_text

    def test_extremely_long_korean_paragraph(self):
        """50,000ì í•œêµ­ì–´ ë‹¨ì¼ ë¬¸ë‹¨ ë¶„í• .

        ì²­ì»¤ëŠ” ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ë¯€ë¡œ, ì¤„ë°”ê¿ˆì´ ìˆëŠ” ê¸´ ë¬¸ë‹¨ì„ í…ŒìŠ¤íŠ¸.
        """
        sentence = "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì€ í˜„ëŒ€ ì‚¬íšŒì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤."
        lines = []
        total_chars = 0
        while total_chars < 50_000:
            lines.append(sentence)
            total_chars += len(sentence) + 1  # +1 for newline
        long_para = "\n".join(lines)
        md = f"# ê¸´ ë¬¸ë‹¨\n\n{long_para}"
        chunks = chunk_markdown(md, source="long.md", max_chunk_size=2000)
        assert len(chunks) >= 25  # 50k / 2k = 25
        total = sum(len(c.content) for c in chunks)
        assert total >= 50_000

    def test_korean_url_and_links(self):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë‚´ URL, ë§ˆí¬ë‹¤ìš´ ë§í¬ ë³´ì¡´."""
        md = """# ì°¸ê³  ìë£Œ

ìì„¸í•œ ë‚´ìš©ì€ [ë„¤ì´ë²„](https://www.naver.com)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

ê´€ë ¨ ë…¼ë¬¸: [í•œêµ­ì–´ NLP ì—°êµ¬](https://arxiv.org/abs/2024.12345)

GitHub ì €ì¥ì†Œ: https://github.com/jedikim/zvecsearch

## ê´€ë ¨ ë§í¬ ëª¨ìŒ

- [ì¹´ì¹´ì˜¤ AI](https://ai.kakao.com) â€” ì¹´ì¹´ì˜¤ì˜ AI ê¸°ìˆ 
- [ë„¤ì´ë²„ í´ë¡œë°”](https://clova.ai) â€” ë„¤ì´ë²„ì˜ AI í”Œë«í¼
"""
        chunks = chunk_markdown(md, source="links.md")
        all_text = " ".join(c.content for c in chunks)
        assert "https://www.naver.com" in all_text
        assert "github.com" in all_text
        assert "ì¹´ì¹´ì˜¤ AI" in all_text

    def test_korean_footnotes_and_references(self):
        """ê°ì£¼, ì°¸ì¡° íŒ¨í„´ ë³´ì¡´."""
        md = """# ì—°êµ¬ ë…¼ë¬¸

ì¸ê³µì§€ëŠ¥ì€ 1956ë…„ ë‹¤íŠ¸ë¨¸ìŠ¤ íšŒì˜ì—ì„œ ì‹œì‘ë˜ì—ˆë‹¤[^1].
ìµœê·¼ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜[^2]ê°€ ìì—°ì–´ì²˜ë¦¬ì— í˜ëª…ì„ ê°€ì ¸ì™”ë‹¤.

[^1]: McCarthy, J. (1956). "The Dartmouth Conference"
[^2]: Vaswani, A. et al. (2017). "Attention Is All You Need"

## ì°¸ê³ ë¬¸í—Œ

1. ê¹€ì² ìˆ˜ (2023). "í•œêµ­ì–´ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê°œë°œ". í•œêµ­ì •ë³´ê³¼í•™íšŒ.
2. ì´ì˜í¬ (2024). "ë²¡í„° ê²€ìƒ‰ ìµœì í™” ê¸°ë²•". AI ì—°êµ¬ì†Œ.
"""
        chunks = chunk_markdown(md, source="refs.md")
        all_text = " ".join(c.content for c in chunks)
        assert "[^1]" in all_text
        assert "ê¹€ì² ìˆ˜" in all_text

    def test_single_char_korean_headings(self):
        """ë‹¨ì¼ í•œê¸€ ê¸€ì í—¤ë”© íŒŒì‹±."""
        md = """# ê°€

ì²« ë²ˆì§¸ ì„¹ì…˜.

## ë‚˜

ë‘ ë²ˆì§¸ ì„¹ì…˜.

### ë‹¤

ì„¸ ë²ˆì§¸ ì„¹ì…˜.
"""
        chunks = chunk_markdown(md, source="single.md")
        headings = [c.heading for c in chunks if c.heading]
        assert "ê°€" in headings
        assert "ë‚˜" in headings
        assert "ë‹¤" in headings

    def test_korean_horizontal_rules_between_sections(self):
        """--- êµ¬ë¶„ì„  ì‚¬ì´ í•œêµ­ì–´ ì„¹ì…˜."""
        md = """# ì²« ë²ˆì§¸ ì„¹ì…˜

ë‚´ìš© 1

---

# ë‘ ë²ˆì§¸ ì„¹ì…˜

ë‚´ìš© 2

---

# ì„¸ ë²ˆì§¸ ì„¹ì…˜

ë‚´ìš© 3
"""
        chunks = chunk_markdown(md, source="rules.md")
        headings = [c.heading for c in chunks if c.heading]
        assert "ì²« ë²ˆì§¸ ì„¹ì…˜" in headings
        assert "ë‘ ë²ˆì§¸ ì„¹ì…˜" in headings
        assert "ì„¸ ë²ˆì§¸ ì„¹ì…˜" in headings


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 6: ëŒ€ê·œëª¨ í•œêµ­ì–´ ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestKoreanLargeScaleIntegration:
    """ëŒ€ê·œëª¨ í•œêµ­ì–´ ë°ì´í„°ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸."""

    @pytest.mark.asyncio
    async def test_200_korean_files_full_pipeline(self, tmp_path):
        """200ê°œ í•œêµ­ì–´ íŒŒì¼ ì „ì²´ íŒŒì´í”„ë¼ì¸."""
        _generate_korean_files(tmp_path, count=200)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count = await zs.index()
            assert count == 200
            assert store.count() == 200

            emb._custom_return = [[0.5] * TEST_DIM]
            results = await zs.search("ì¸ê³µì§€ëŠ¥", top_k=20)
            assert len(results) == 20
            zs.close()

    @pytest.mark.asyncio
    async def test_large_korean_document_1000_lines(self, tmp_path):
        """1000ì¤„ í•œêµ­ì–´ ë¬¸ì„œ ì²­í‚¹ + ì¸ë±ì‹± + ê²€ìƒ‰."""
        lines = ["# ëŒ€ê·œëª¨ í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ\n"]
        for i in range(50):
            topic = _KOREAN_TOPICS[i % len(_KOREAN_TOPICS)]
            lines.append(f"\n## ì„¹ì…˜ {i + 1}: {topic}\n")
            for j in range(19):
                sentence = _KOREAN_SENTENCES[(i + j) % len(_KOREAN_SENTENCES)]
                lines.append(f"{sentence}\n")
        md = "\n".join(lines)
        assert md.count("\n") >= 1000

        (tmp_path / "ëŒ€ê·œëª¨ë¬¸ì„œ.md").write_text(md)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count = await zs.index()
            assert count >= 50  # ìµœì†Œ 50ê°œ ì„¹ì…˜
            assert len(emb._embed_calls) >= 1
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_search_result_ordering(self, tmp_path):
        """ë‹¤ì–‘í•œ top_kë¡œ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì œí•œ ì¤€ìˆ˜."""
        _generate_korean_files(tmp_path, count=50)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            await zs.index()

            emb._custom_return = [[0.5] * TEST_DIM]
            for k in [1, 3, 5, 10, 25, 50]:
                results = await zs.search("ë¨¸ì‹ ëŸ¬ë‹", top_k=k)
                assert len(results) <= k
                assert all(isinstance(r["score"], float) for r in results)
            zs.close()

    @pytest.mark.asyncio
    async def test_concurrent_korean_file_changes(self, tmp_path):
        """5íšŒ ì‚¬ì´í´(íŒŒì¼ ì¶”ê°€+ìˆ˜ì •+ì‚­ì œ) â†’ ìµœì¢… ìƒíƒœ."""
        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")

            live_files = set()
            for cycle in range(5):
                # 3ê°œ íŒŒì¼ ì¶”ê°€
                for j in range(3):
                    name = f"ì‚¬ì´í´{cycle}_ë¬¸ì„œ{j}.md"
                    (tmp_path / name).write_text(
                        f"# ì‚¬ì´í´ {cycle} ë¬¸ì„œ {j}\n\në‚´ìš©ì…ë‹ˆë‹¤."
                    )
                    live_files.add(name)

                await zs.index()

                # ì´ì „ ì‚¬ì´í´ íŒŒì¼ 1ê°œ ìˆ˜ì • (ìˆëŠ” ê²½ìš°)
                if cycle > 0:
                    modify_name = f"ì‚¬ì´í´{cycle - 1}_ë¬¸ì„œ0.md"
                    if (tmp_path / modify_name).exists():
                        (tmp_path / modify_name).write_text(
                            f"# ìˆ˜ì •ë¨ ì‚¬ì´í´ {cycle}\n\nìˆ˜ì •ëœ ë‚´ìš©."
                        )

                await zs.index()

            # ìµœì¢…: 15ê°œ íŒŒì¼ì´ ìƒì„±ë¨
            assert len(live_files) == 15
            assert store.count() == 15
            zs.close()

    @pytest.mark.asyncio
    async def test_korean_content_utf8_roundtrip(self, tmp_path):
        """í•œêµ­ì–´ â†’ ì²­í‚¹ â†’ ì €ì¥ â†’ ì¡°íšŒ â†’ ì›ë³¸ ì¼ì¹˜."""
        original = "# ìœ ë‹ˆì½”ë“œ ë¼ìš´ë“œíŠ¸ë¦½\n\ní•œê¸€ ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜. " \
                   "íŠ¹ìˆ˜ë¬¸ì: â€»â˜…â—†â– â—â†’. ìëª¨: ã„±ã„´ã„·ã„¹ã…ã…‚ã……. " \
                   "ì´ëª¨ì§€: ğŸ‰ğŸš€ğŸ’¡. ì¼ë³¸ì–´: ã“ã‚“ã«ã¡ã¯. ì¤‘êµ­ì–´: ä½ å¥½ä¸–ç•Œ."
        (tmp_path / "roundtrip.md").write_text(original)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            await zs.index()

            stored_content = list(store._docs.values())[0]["content"]
            assert "í•œê¸€ ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜" in stored_content
            assert "â€»â˜…â—†â– â—â†’" in stored_content
            assert "ã„±ã„´ã„·ã„¹ã…ã…‚ã……" in stored_content
            assert "ğŸ‰ğŸš€ğŸ’¡" in stored_content
            assert "ã“ã‚“ã«ã¡ã¯" in stored_content
            assert "ä½ å¥½ä¸–ç•Œ" in stored_content
            zs.close()


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 7: í•œêµ­ì–´ CLI ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestKoreanCLIStress:
    """í•œêµ­ì–´ í™˜ê²½ì—ì„œì˜ CLI ë™ì‘ í…ŒìŠ¤íŠ¸."""

    def test_config_set_get_korean_collection(self):
        """í•œêµ­ì–´ ì»¬ë ‰ì…˜ëª… ì„¤ì •/ì¡°íšŒ."""
        from click.testing import CliRunner
        from zvecsearch.cli import cli

        runner = CliRunner()
        # config getìœ¼ë¡œ ê¸°ë³¸ê°’ í™•ì¸
        result = runner.invoke(cli, ["config", "get", "zvec.collection"])
        assert result.exit_code == 0
        assert "zvecsearch_chunks" in result.output

    def test_help_output_all_commands(self):
        """ëª¨ë“  CLI ëª…ë ¹ì–´ --help ì •ìƒ ë™ì‘."""
        from click.testing import CliRunner
        from zvecsearch.cli import cli

        runner = CliRunner()
        commands = ["index", "search", "watch", "compact", "stats",
                     "reset", "expand", "transcript", "config"]
        for cmd in commands:
            result = runner.invoke(cli, [cmd, "--help"])
            assert result.exit_code == 0, f"'{cmd} --help' ì‹¤íŒ¨: {result.output}"

    def test_stats_command_output_format(self):
        """stats ëª…ë ¹ì–´ ì¶œë ¥ í˜•ì‹ í™•ì¸."""
        from click.testing import CliRunner
        from zvecsearch.cli import cli

        runner = CliRunner()
        # stats without valid store should handle gracefully
        result = runner.invoke(cli, ["stats", "--zvec-path", "/tmp/nonexistent_korean_test"])
        # Either succeeds or fails gracefully (not a crash)
        assert result.exit_code in (0, 1, 2)

    def test_config_list_all_sections(self):
        """config listê°€ ëª¨ë“  ì„¤ì • ì„¹ì…˜ í¬í•¨."""
        from click.testing import CliRunner
        from zvecsearch.cli import cli

        runner = CliRunner()
        result = runner.invoke(cli, ["config", "list", "--resolved"])
        assert result.exit_code == 0
        for section in ["zvec", "index", "embedding", "compact", "chunking", "watch"]:
            assert section in result.output, f"ì„¹ì…˜ '{section}'ì´ ì¶œë ¥ì— ì—†ìŒ"
