"""ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸.

ë‹¤ìŒì„ ê²€ì¦í•©ë‹ˆë‹¤:
- ìˆ˜ë°± ê°œì˜ íŒŒì¼/ìˆ˜ì²œ ê°œì˜ ì²­í¬ë¥¼ ì‚¬ìš©í•œ ëŒ€ê·œëª¨ ì¸ë±ì‹±
- ê¹Šì€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìŠ¤ìºë‹
- ìœ ë‹ˆì½”ë“œ/ë‹¤êµ­ì–´ ì½˜í…ì¸  ì²˜ë¦¬
- ë³µì¡í•œ ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° (6ë‹¨ê³„ í—¤ë”©, ì½”ë“œ ë¸”ë¡, í…Œì´ë¸”)
- ì¦ë¶„ ì¸ë±ì‹± ì •í™•ì„± (ìˆ˜ì •/ì‚­ì œ/ì¶”ê°€)
- í•´ì‹œ ì¶©ëŒ ì—†ìŒ ê²€ì¦
- ì²­í‚¹ ê²½ê³„ ì¡°ê±´ (ë§¤ìš° ê¸´ ì¤„, ë¹ˆ ì„¹ì…˜, íŠ¹ìˆ˜ ë¬¸ì)
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ (scan â†’ chunk â†’ embed â†’ store â†’ search)
"""
from __future__ import annotations

import random
import sys
from unittest.mock import MagicMock, patch

import pytest

# ---------------------------------------------------------------------------
# zvec ëª¨ë“ˆ ìŠ¤í… (ë„¤ì´í‹°ë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ AVX-512 í•„ìš”)
# ---------------------------------------------------------------------------
if "zvec" not in sys.modules:
    _zvec_stub = MagicMock()
    _zvec_stub.DataType.STRING = "STRING"
    _zvec_stub.DataType.INT32 = "INT32"
    _zvec_stub.DataType.VECTOR_FP32 = "VECTOR_FP32"
    _zvec_stub.DataType.SPARSE_VECTOR_FP32 = "SPARSE_VECTOR_FP32"
    _zvec_stub.MetricType.COSINE = "COSINE"
    _zvec_stub.MetricType.L2 = "L2"
    _zvec_stub.MetricType.IP = "IP"
    _zvec_stub.LogLevel.WARN = "WARN"
    _zvec_stub.FieldSchema = MagicMock
    _zvec_stub.VectorSchema = MagicMock
    _zvec_stub.CollectionSchema = MagicMock
    _zvec_stub.CollectionOption = MagicMock
    _zvec_stub.HnswIndexParam = MagicMock
    _zvec_stub.InvertIndexParam = MagicMock
    _zvec_stub.FlatIndexParam = MagicMock
    _zvec_stub.VectorQuery = MagicMock
    _zvec_stub.RrfReRanker = MagicMock
    _zvec_stub.BM25EmbeddingFunction = MagicMock
    _zvec_stub.Doc = MagicMock
    sys.modules["zvec"] = _zvec_stub

from zvecsearch.chunker import Chunk, chunk_markdown, compute_chunk_id  # noqa: E402
from zvecsearch.config import (  # noqa: E402
    ZvecSearchConfig,
    deep_merge,
    resolve_config,
    config_to_dict,
    load_config_file,
    save_config,
    get_config_value,
)
from zvecsearch.core import ZvecSearch  # noqa: E402
from zvecsearch.scanner import scan_paths  # noqa: E402

# ---------------------------------------------------------------------------
# ìƒìˆ˜
# ---------------------------------------------------------------------------
TEST_DIM = 8
LARGE_FILE_COUNT = 50
LARGE_HEADINGS_PER_FILE = 20


# ---------------------------------------------------------------------------
# ìœ í‹¸ë¦¬í‹°: ìƒíƒœ ê¸°ë°˜ ìŠ¤í† ì–´ Mock
# ---------------------------------------------------------------------------
def _make_stateful_store() -> MagicMock:
    """ìƒíƒœ ê¸°ë°˜ ì¸ë©”ëª¨ë¦¬ ZvecStore Mock."""
    store = MagicMock()
    store._docs: dict[str, dict] = {}

    def _upsert(chunks):
        for c in chunks:
            store._docs[c["chunk_hash"]] = c
        return len(chunks)

    store.upsert.side_effect = _upsert
    store.count.side_effect = lambda: len(store._docs)

    def _hashes_by_source(source):
        return {h for h, c in store._docs.items() if c.get("source") == source}

    store.hashes_by_source.side_effect = _hashes_by_source

    def _existing_hashes(hashes):
        return {h for h in hashes if h in store._docs}

    store.existing_hashes.side_effect = _existing_hashes

    def _delete_by_hashes(hashes):
        for h in hashes:
            store._docs.pop(h, None)

    store.delete_by_hashes.side_effect = _delete_by_hashes

    def _delete_by_source(source):
        to_del = [h for h, c in store._docs.items() if c.get("source") == source]
        for h in to_del:
            del store._docs[h]

    store.delete_by_source.side_effect = _delete_by_source

    def _search(query_embedding, query_text="", top_k=10, filter_expr=""):
        results = list(store._docs.values())[:top_k]
        return [
            {
                "content": c.get("content", ""),
                "source": c.get("source", ""),
                "heading": c.get("heading", ""),
                "heading_level": c.get("heading_level", 0),
                "start_line": c.get("start_line", 0),
                "end_line": c.get("end_line", 0),
                "chunk_hash": c.get("chunk_hash", ""),
                "score": 0.95 - i * 0.01,
            }
            for i, c in enumerate(results)
        ]

    store.search.side_effect = _search
    store.close.return_value = None
    store.drop.return_value = None
    return store


class FakeEmbedder:
    """Mock ì˜¤ì—¼ì„ í”¼í•˜ê¸° ìœ„í•œ ìˆœìˆ˜ Python ì„ë² ë”© ì œê³µì.

    test_store.pyì˜ BM25 Mockì´ MagicMock í´ë˜ìŠ¤ë¥¼ ì˜¤ì—¼ì‹œí‚¤ë¯€ë¡œ,
    AsyncMock ëŒ€ì‹  ìˆœìˆ˜ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì™„ì „íˆ ê²©ë¦¬.
    """

    def __init__(self, dim: int = TEST_DIM):
        self.model_name = "stress-test-model"
        self.dimension = dim
        self._dim = dim
        self._embed_calls: list[list[str]] = []
        self._custom_return = None

    async def embed(self, texts: list[str]) -> list[list[float]]:
        self._embed_calls.append(texts)
        if self._custom_return is not None:
            return self._custom_return
        return [[float(i + 1) * 0.1] * self._dim for i, _ in enumerate(texts)]


def _make_embedder(dim: int = TEST_DIM) -> FakeEmbedder:
    """ê²°ì •ì  ë²¡í„°ë¥¼ ë°˜í™˜í•˜ëŠ” ì„ë² ë”© ì œê³µì."""
    return FakeEmbedder(dim=dim)


# ---------------------------------------------------------------------------
# ëŒ€ê·œëª¨ ë§ˆí¬ë‹¤ìš´ ìƒì„± ìœ í‹¸ë¦¬í‹°
# ---------------------------------------------------------------------------
def _generate_large_markdown(
    num_headings: int = 20,
    paragraphs_per_heading: int = 3,
    words_per_paragraph: int = 80,
) -> str:
    """ìˆ˜ë§ì€ í—¤ë”©ê³¼ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±ëœ ëŒ€í˜• ë§ˆí¬ë‹¤ìš´ ìƒì„±."""
    parts = []
    for i in range(num_headings):
        level = (i % 4) + 1  # h1~h4 ìˆœí™˜
        parts.append(f"{'#' * level} ì„¹ì…˜ {i}: ì£¼ì œ {chr(65 + i % 26)}")
        for p in range(paragraphs_per_heading):
            words = " ".join(
                f"ë‹¨ì–´{random.randint(100, 999)}" for _ in range(words_per_paragraph)
            )
            parts.append(f"\n{words}\n")
    return "\n".join(parts)


def _generate_unicode_markdown() -> str:
    """ë‹¤ì–‘í•œ ìœ ë‹ˆì½”ë“œ/ë‹¤êµ­ì–´ ì½˜í…ì¸ ê°€ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´ ìƒì„±."""
    return """# í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ

ì´ê²ƒì€ í•œêµ­ì–´ë¡œ ì‘ì„±ëœ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤.
ì‹œë§¨í‹± ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ë‹¤êµ­ì–´ ì§€ì›ì„ ê²€ì¦í•©ë‹ˆë‹¤.

## æ—¥æœ¬èªã‚»ã‚¯ã‚·ãƒ§ãƒ³

ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚
ãƒãƒ«ãƒãƒã‚¤ãƒˆæ–‡å­—ã®ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

## ä¸­æ–‡éƒ¨åˆ†

è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡æµ‹è¯•éƒ¨åˆ†ã€‚
ç”¨äºéªŒè¯å¤šè¯­è¨€æ”¯æŒå’ŒUnicodeå¤„ç†ã€‚

## Emojis & Symbols ğŸš€

Special characters: Ã©, Ã±, Ã¼, Ã¸, ÃŸ, Ã¦, Î¸, Î», Ï€, Î£
Math: âˆ«âˆ‘âˆâˆšâˆÂ±â‰ â‰ˆâ‰¤â‰¥
Arrows: â†â†’â†‘â†“â†”â‡’â‡â‡‘â‡“
Box drawing: â”Œâ”€â”â”‚â””â”€â”˜â”œâ”¤â”¬â”´â”¼

## ì½”ë“œ ë¸”ë¡ í…ŒìŠ¤íŠ¸

```python
def ì¸ì‚¬(ì´ë¦„: str) -> str:
    return f"ì•ˆë…•í•˜ì„¸ìš”, {ì´ë¦„}ë‹˜!"

# ìœ ë‹ˆì½”ë“œ ë³€ìˆ˜ëª…ë„ ì§€ì›í•©ë‹ˆë‹¤
ê²°ê³¼ = ì¸ì‚¬("ì„¸ê³„")
print(ê²°ê³¼)
```

## í…Œì´ë¸” í…ŒìŠ¤íŠ¸

| ì´ë¦„ | ë‚˜ì´ | ë„ì‹œ |
|------|------|------|
| ê¹€ì² ìˆ˜ | 25 | ì„œìš¸ |
| ì´ì˜í¬ | 30 | ë¶€ì‚° |
| ë°•ì§€ë¯¼ | 28 | ëŒ€êµ¬ |

## í˜¼í•© ì½˜í…ì¸ 

> ì¸ìš©ë¬¸: "ì§€ì‹ì€ í˜ì´ë‹¤." - í”„ëœì‹œìŠ¤ ë² ì´ì»¨

- ëª©ë¡ í•­ëª© 1: ì²« ë²ˆì§¸
- ëª©ë¡ í•­ëª© 2: ë‘ ë²ˆì§¸
  - í•˜ìœ„ í•­ëª©: ì¤‘ì²©ëœ ëª©ë¡

1. ë²ˆí˜¸ ëª©ë¡ 1
2. ë²ˆí˜¸ ëª©ë¡ 2
3. ë²ˆí˜¸ ëª©ë¡ 3

---

ë§ˆì§€ë§‰ ë‹¨ë½. ì´ ë¬¸ì„œëŠ” ë‹¤ì–‘í•œ ìœ ë‹ˆì½”ë“œ ë¬¸ìì™€ ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""


def _generate_complex_nested_markdown() -> str:
    """6ë‹¨ê³„ í—¤ë”©, ì½”ë“œ ë¸”ë¡, ì¸ë¼ì¸ ì½”ë“œê°€ í¬í•¨ëœ ë³µì¡í•œ ë§ˆí¬ë‹¤ìš´."""
    return """# Level 1: ì•„í‚¤í…ì²˜ ê°œìš”

ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ìµœìƒìœ„ ì„¤ëª…ì…ë‹ˆë‹¤.

## Level 2: ë°±ì—”ë“œ ì‹œìŠ¤í…œ

ë°±ì—”ë“œëŠ” Pythonìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.

### Level 3: ë°ì´í„°ë² ì´ìŠ¤ ê³„ì¸µ

ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ zvecë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### Level 4: ì¸ë±ì‹± ì „ëµ

HNSW ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰.

##### Level 5: íŒŒë¼ë¯¸í„° íŠœë‹

`ef_construction=300`, `max_m=16` ì„¤ì • ì‚¬ìš©.

###### Level 6: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

| ë°ì´í„°ì…‹ | ì°¨ì› | QPS | Recall@10 |
|----------|------|-----|-----------|
| 1M vectors | 768 | 5000 | 0.98 |
| 10M vectors | 768 | 2000 | 0.95 |

## Level 2: í”„ë¡ íŠ¸ì—”ë“œ

CLI ì¸í„°í˜ì´ìŠ¤ë¡œ Click í”„ë ˆì„ì›Œí¬ ì‚¬ìš©.

### Level 3: ëª…ë ¹ì–´ êµ¬ì¡°

```bash
zvecsearch index ./docs/
zvecsearch search "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë€?"
zvecsearch watch ./docs/ --debounce-ms 2000
```

### Level 3: ì„¤ì • ê´€ë¦¬

TOML ê¸°ë°˜ ê³„ì¸µì  ì„¤ì • ì‹œìŠ¤í…œ:
- ê¸€ë¡œë²Œ: `~/.zvecsearch/config.toml`
- í”„ë¡œì íŠ¸: `.zvecsearch.toml`
- CLI ì˜¤ë²„ë¼ì´ë“œ

#### Level 4: ì„¤ì • ì˜ˆì‹œ

```toml
[zvec]
path = "~/.zvecsearch/db"
collection = "my_knowledge"

[embedding]
provider = "openai"
model = "text-embedding-3-small"
```

## Level 2: ì„ë² ë”© ì‹œìŠ¤í…œ

5ê°œ ì„ë² ë”© ì œê³µì ì§€ì›.

### Level 3: OpenAI

ê¸°ë³¸ ì œê³µì, `text-embedding-3-small` ëª¨ë¸.

### Level 3: ë¡œì»¬ ëª¨ë¸

`sentence-transformers` ê¸°ë°˜, ì˜¤í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥.

# Level 1: ë°°í¬ ê°€ì´ë“œ

## Level 2: ì„¤ì¹˜

```bash
pip install zvecsearch
pip install "zvecsearch[all]"  # ëª¨ë“  ì˜ì¡´ì„±
```

## Level 2: ì—…ê·¸ë ˆì´ë“œ

ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 1: ì²­ì»¤ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestChunkerStress:
    """ëŒ€ê·œëª¨ ë° ë³µì¡í•œ ë§ˆí¬ë‹¤ìš´ì— ëŒ€í•œ ì²­í‚¹ í…ŒìŠ¤íŠ¸."""

    def test_large_document_produces_many_chunks(self):
        """20ê°œ í—¤ë”©ì´ ìˆëŠ” ëŒ€í˜• ë¬¸ì„œê°€ ì˜¬ë°”ë¥´ê²Œ ì²­í‚¹ë˜ëŠ”ì§€ ê²€ì¦."""
        md = _generate_large_markdown(num_headings=20, paragraphs_per_heading=3)
        chunks = chunk_markdown(md, source="large.md", max_chunk_size=500)
        # í° ì„¹ì…˜ì´ ë¶„í• ë˜ë¯€ë¡œ ìµœì†Œ 20ê°œ ì´ìƒì˜ ì²­í¬ ì˜ˆìƒ
        assert len(chunks) >= 20
        # ëª¨ë“  ì²­í¬ì— ì½˜í…ì¸ ê°€ ìˆì–´ì•¼ í•¨
        assert all(c.content.strip() for c in chunks)
        # ëª¨ë“  ì²­í¬ê°€ max_chunk_sizeë³´ë‹¤ ì‘ê±°ë‚˜ í•©ë¦¬ì  ë²”ìœ„ ì´ë‚´
        for c in chunks:
            assert len(c.content) < 2000  # ì•½ê°„ì˜ ì—¬ìœ  í—ˆìš©

    def test_50_heading_document(self):
        """50ê°œ í—¤ë”© ë¬¸ì„œì˜ ì²­í‚¹ ì •í™•ì„±."""
        md = _generate_large_markdown(num_headings=50, paragraphs_per_heading=1, words_per_paragraph=30)
        chunks = chunk_markdown(md, source="fifty.md")
        assert len(chunks) >= 50
        # ê° ì²­í¬ì˜ start_lineì´ ë‹¨ì¡° ì¦ê°€
        for i in range(1, len(chunks)):
            assert chunks[i].start_line >= chunks[i - 1].start_line

    def test_unicode_content_chunking(self):
        """ìœ ë‹ˆì½”ë“œ/ë‹¤êµ­ì–´ ì½˜í…ì¸  ì²­í‚¹."""
        md = _generate_unicode_markdown()
        chunks = chunk_markdown(md, source="unicode.md")
        assert len(chunks) >= 7  # ìµœì†Œ 7ê°œ ì„¹ì…˜
        # í•œêµ­ì–´ ì½˜í…ì¸  í¬í•¨ í™•ì¸
        korean_chunks = [c for c in chunks if "í•œêµ­ì–´" in c.content or "í…ŒìŠ¤íŠ¸" in c.content]
        assert len(korean_chunks) >= 1
        # ì¼ë³¸ì–´ ì½˜í…ì¸  í¬í•¨ í™•ì¸
        japanese_chunks = [c for c in chunks if "æ—¥æœ¬èª" in c.content]
        assert len(japanese_chunks) >= 1
        # ì´ëª¨ì§€ ì½˜í…ì¸  í¬í•¨ í™•ì¸
        emoji_chunks = [c for c in chunks if "ğŸš€" in c.content]
        assert len(emoji_chunks) >= 1

    def test_six_level_heading_hierarchy(self):
        """6ë‹¨ê³„ í—¤ë”© ê³„ì¸µ êµ¬ì¡° ì²˜ë¦¬."""
        md = _generate_complex_nested_markdown()
        chunks = chunk_markdown(md, source="complex.md")
        levels = {c.heading_level for c in chunks if c.heading_level > 0}
        # h1~h6ê¹Œì§€ ëª¨ë“  ë ˆë²¨ì´ ìˆì–´ì•¼ í•¨
        assert levels == {1, 2, 3, 4, 5, 6}

    def test_code_block_heading_is_known_limitation(self):
        """ì½”ë“œ ë¸”ë¡ ì•ˆì˜ # ë¬¸ìê°€ í—¤ë”©ìœ¼ë¡œ ì¸ì‹ë˜ëŠ” ê²ƒì€ ì•Œë ¤ì§„ ì œí•œ ì‚¬í•­.

        í˜„ì¬ ì²­ì»¤ëŠ” ì½”ë“œ ë¸”ë¡ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
        ì½”ë“œ ë¸”ë¡ ì•ˆì˜ '#'ë„ í—¤ë”©ìœ¼ë¡œ ë¶„í• ë¨. ì´ê²ƒì€ memsearchì™€ ë™ì¼í•œ ë™ì‘.
        """
        md = "# Real Heading\n\n```python\n# This is a comment\ndef foo():\n    pass\n```\n\n## Another"
        chunks = chunk_markdown(md, source="code.md")
        headings = [c.heading for c in chunks if c.heading]
        assert "Real Heading" in headings
        assert "Another" in headings
        # ì•Œë ¤ì§„ ì œí•œ: ì½”ë“œ ë¸”ë¡ ë‚´ #ë„ í—¤ë”©ìœ¼ë¡œ ì¸ì‹ë¨
        assert len(chunks) >= 2

    def test_very_long_single_line(self):
        """10,000ì ë‹¨ì¼ ì¤„ ì²˜ë¦¬."""
        long_line = "x" * 10_000
        md = f"# Long\n{long_line}"
        chunks = chunk_markdown(md, source="long.md", max_chunk_size=2000)
        assert len(chunks) >= 1
        # ì „ì²´ ì½˜í…ì¸ ê°€ ë³´ì¡´ë˜ì–´ì•¼ í•¨
        total_chars = sum(len(c.content) for c in chunks)
        assert total_chars >= 10_000

    def test_empty_sections_between_headings(self):
        """í—¤ë”© ì‚¬ì´ì— ë¹ˆ ì„¹ì…˜ì´ ìˆëŠ” ê²½ìš°."""
        md = "# A\n\n# B\n\n# C\nContent here."
        chunks = chunk_markdown(md, source="empty.md")
        # ë¹ˆ ì„¹ì…˜ì€ ê±´ë„ˆë›°ì–´ì•¼ í•¨
        content_chunks = [c for c in chunks if c.content.strip()]
        assert all(c.content.strip() for c in content_chunks)

    def test_chunk_hash_uniqueness_at_scale(self):
        """ëŒ€ê·œëª¨ ì²­í‚¹ ì‹œ í•´ì‹œ ê³ ìœ ì„± ê²€ì¦.

        ì˜¤ë²„ë© ë¶„í•  ì‹œ ë™ì¼ ì½˜í…ì¸ ê°€ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        ê³ ìœ  ì½˜í…ì¸ ë¥¼ ê°€ì§„ ì²­í¬ëŠ” ê³ ìœ  í•´ì‹œë¥¼ ê°€ì ¸ì•¼ í•¨.
        """
        md = _generate_large_markdown(num_headings=100, paragraphs_per_heading=2, words_per_paragraph=50)
        chunks = chunk_markdown(md, source="unique.md", max_chunk_size=200)
        # ë™ì¼ ì½˜í…ì¸  â†’ ë™ì¼ í•´ì‹œ (ì •ìƒ ë™ì‘)
        content_to_hash = {}
        for c in chunks:
            if c.content in content_to_hash:
                assert c.content_hash == content_to_hash[c.content], "ê°™ì€ ì½˜í…ì¸ ì¸ë° ë‹¤ë¥¸ í•´ì‹œ"
            else:
                content_to_hash[c.content] = c.content_hash
        # ê³ ìœ  ì½˜í…ì¸  ìˆ˜ì™€ ê³ ìœ  í•´ì‹œ ìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•¨
        unique_contents = set(c.content for c in chunks)
        unique_hashes = set(c.content_hash for c in chunks)
        assert len(unique_contents) == len(unique_hashes), "ê³ ìœ  ì½˜í…ì¸ ì™€ ê³ ìœ  í•´ì‹œ ìˆ˜ê°€ ë¶ˆì¼ì¹˜"

    def test_chunk_id_varies_with_model(self):
        """ê°™ì€ ì²­í¬ë¼ë„ ëª¨ë¸ì´ ë‹¤ë¥´ë©´ chunk_idê°€ ë‹¬ë¼ì•¼ í•¨."""
        ids_per_model = {}
        for model in ["openai", "google", "voyage", "ollama", "local"]:
            chunk_id = compute_chunk_id("test.md", 1, 10, "abc123", model)
            ids_per_model[model] = chunk_id
        # ëª¨ë“  ëª¨ë¸ì˜ IDê°€ ì„œë¡œ ë‹¬ë¼ì•¼ í•¨
        assert len(set(ids_per_model.values())) == 5

    def test_overlap_produces_shared_content(self):
        """ì˜¤ë²„ë© ì„¤ì •ì´ ì‹¤ì œë¡œ ê²¹ì¹˜ëŠ” ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ëŠ”ì§€ ê²€ì¦."""
        lines = ["# Big Section"]
        for i in range(50):
            lines.append(f"Line {i}: " + "word " * 30)
        md = "\n".join(lines)
        chunks = chunk_markdown(md, source="overlap.md", max_chunk_size=300, overlap_lines=3)
        if len(chunks) >= 2:
            # ì¸ì ‘ ì²­í¬ ê°„ ì¼ë¶€ ì¤„ì´ ê²¹ì³ì•¼ í•¨
            first_lines = set(chunks[0].content.split("\n"))
            second_lines = set(chunks[1].content.split("\n"))
            overlap = first_lines & second_lines
            # ë¹ˆ ì¤„ ì œì™¸í•˜ê³  ê²¹ì¹˜ëŠ” ì¤„ì´ ìˆì–´ì•¼ í•¨
            meaningful_overlap = {line for line in overlap if line.strip()}
            assert len(meaningful_overlap) >= 1, "ì˜¤ë²„ë©ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ"


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 2: ìŠ¤ìºë„ˆ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestScannerStress:
    """ëŒ€ê·œëª¨ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìŠ¤ìºë‹ í…ŒìŠ¤íŠ¸."""

    def test_scan_100_files(self, tmp_path):
        """100ê°œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìŠ¤ìºë‹."""
        for i in range(100):
            (tmp_path / f"doc_{i:03d}.md").write_text(f"# Doc {i}\nContent {i}")
        results = scan_paths([tmp_path])
        assert len(results) == 100

    def test_deeply_nested_directories(self, tmp_path):
        """10ë‹¨ê³„ ê¹Šì´ì˜ ì¤‘ì²© ë””ë ‰í† ë¦¬."""
        current = tmp_path
        for depth in range(10):
            current = current / f"level_{depth}"
            current.mkdir()
            (current / f"doc_depth_{depth}.md").write_text(f"# Depth {depth}")
        results = scan_paths([tmp_path])
        assert len(results) == 10

    def test_mixed_extensions(self, tmp_path):
        """ë‹¤ì–‘í•œ í™•ì¥ìê°€ í˜¼í•©ëœ ë””ë ‰í† ë¦¬."""
        for ext in [".md", ".markdown", ".txt", ".py", ".json", ".html", ".rst"]:
            for i in range(10):
                (tmp_path / f"file_{i}{ext}").write_text(f"Content {i}")
        results = scan_paths([tmp_path])
        # .mdì™€ .markdownë§Œ í¬í•¨ (ê° 10ê°œ)
        assert len(results) == 20

    def test_hidden_files_and_dirs(self, tmp_path):
        """ìˆ¨ê¹€ íŒŒì¼/ë””ë ‰í† ë¦¬ ë¬´ì‹œ ê²€ì¦."""
        (tmp_path / "visible.md").write_text("# Visible")
        (tmp_path / ".hidden.md").write_text("# Hidden")
        hidden_dir = tmp_path / ".hidden_dir"
        hidden_dir.mkdir()
        (hidden_dir / "inside.md").write_text("# Inside hidden")
        results = scan_paths([tmp_path])
        assert len(results) == 1
        assert results[0].path.name == "visible.md"

    def test_symlink_handling(self, tmp_path):
        """ì‹¬ë³¼ë¦­ ë§í¬ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬."""
        real_dir = tmp_path / "real"
        real_dir.mkdir()
        (real_dir / "a.md").write_text("# A")
        link_dir = tmp_path / "link"
        link_dir.symlink_to(real_dir)
        results = scan_paths([tmp_path])
        # ì‹¤ì œ íŒŒì¼ê³¼ ë§í¬ëœ íŒŒì¼ ëª¨ë‘ ë°œê²¬ (ì¤‘ë³µ ì œê±°ë¨)
        paths = {r.path for r in results}
        assert len(paths) >= 1

    def test_unicode_filenames(self, tmp_path):
        """ìœ ë‹ˆì½”ë“œ íŒŒì¼ëª… ì²˜ë¦¬."""
        (tmp_path / "í•œêµ­ì–´ë¬¸ì„œ.md").write_text("# í•œêµ­ì–´")
        (tmp_path / "æ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ.md").write_text("# æ—¥æœ¬èª")
        (tmp_path / "Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚.md").write_text("# Ğ ÑƒÑÑĞºĞ¸Ğ¹")
        results = scan_paths([tmp_path])
        assert len(results) == 3

    def test_large_files(self, tmp_path):
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ (100KB+) ìŠ¤ìºë‹."""
        large_content = "# Big\n" + "x" * 100_000
        (tmp_path / "large.md").write_text(large_content)
        results = scan_paths([tmp_path])
        assert len(results) == 1
        assert results[0].size > 100_000

    def test_dedup_across_multiple_paths(self, tmp_path):
        """ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ë™ì¼ íŒŒì¼ ì¤‘ë³µ ì œê±°."""
        sub1 = tmp_path / "a"
        sub1.mkdir()
        (sub1 / "shared.md").write_text("# Shared")
        results = scan_paths([sub1, sub1 / "shared.md", tmp_path])
        paths = [r.path for r in results]
        assert len(paths) == len(set(paths))


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 3: ì„¤ì • ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestConfigStress:
    """ì„¤ì • ì‹œìŠ¤í…œ ê²½ê³„ ì¡°ê±´ í…ŒìŠ¤íŠ¸."""

    def test_deep_merge_deeply_nested(self):
        """5ë‹¨ê³„ ê¹Šì´ì˜ ì¤‘ì²© ë”•ì…”ë„ˆë¦¬ ë³‘í•©."""
        base = {"a": {"b": {"c": {"d": {"e": 1}}}}}
        over = {"a": {"b": {"c": {"d": {"e": 99, "f": 100}}}}}
        result = deep_merge(base, over)
        assert result["a"]["b"]["c"]["d"]["e"] == 99
        assert result["a"]["b"]["c"]["d"]["f"] == 100

    def test_config_round_trip(self, tmp_path):
        """ì„¤ì • ì €ì¥ í›„ ë‹¤ì‹œ ë¡œë“œí•˜ë©´ ë™ì¼í•´ì•¼ í•¨."""
        original = config_to_dict(ZvecSearchConfig())
        f = tmp_path / "cfg.toml"
        save_config(original, f)
        loaded = load_config_file(f)
        # ëª¨ë“  ìµœìƒìœ„ í‚¤ê°€ ì¡´ì¬í•´ì•¼ í•¨
        for key in ["zvec", "index", "embedding", "compact", "chunking", "watch"]:
            assert key in loaded

    def test_resolve_with_many_overrides(self):
        """ì—¬ëŸ¬ ì„¤ì • ê°’ì„ ë™ì‹œì— ì˜¤ë²„ë¼ì´ë“œ."""
        cfg = resolve_config({
            "zvec": {"path": "/custom/path", "collection": "custom_col"},
            "index": {"metric": "l2", "hnsw_ef": 500},
            "embedding": {"provider": "ollama", "model": "custom-model"},
            "chunking": {"max_chunk_size": 3000, "overlap_lines": 5},
            "watch": {"debounce_ms": 3000},
        })
        assert cfg.zvec.path == "/custom/path"
        assert cfg.zvec.collection == "custom_col"
        assert cfg.index.metric == "l2"
        assert cfg.index.hnsw_ef == 500
        assert cfg.embedding.provider == "ollama"
        assert cfg.embedding.model == "custom-model"
        assert cfg.chunking.max_chunk_size == 3000
        assert cfg.watch.debounce_ms == 3000

    def test_get_config_value_all_paths(self):
        """ëª¨ë“  ì„¤ì • ê²½ë¡œì— ëŒ€í•œ ê°’ ì¡°íšŒ."""
        cfg = ZvecSearchConfig()
        paths_and_expected = [
            ("zvec.path", "~/.zvecsearch/db"),
            ("zvec.collection", "zvecsearch_chunks"),
            ("zvec.enable_mmap", True),
            ("index.type", "hnsw"),
            ("index.metric", "cosine"),
            ("index.hnsw_ef", 300),
            ("index.hnsw_max_m", 16),
            ("embedding.provider", "openai"),
            ("chunking.max_chunk_size", 1500),
            ("chunking.overlap_lines", 2),
            ("watch.debounce_ms", 1500),
        ]
        for path, expected in paths_and_expected:
            assert get_config_value(path, cfg) == expected, f"ì‹¤íŒ¨: {path}"

    def test_deep_merge_preserves_unrelated_keys(self):
        """ë³‘í•© ì‹œ ê´€ë ¨ ì—†ëŠ” í‚¤ê°€ ë³´ì¡´ë˜ëŠ”ì§€ ê²€ì¦."""
        base = {"a": 1, "b": 2, "c": {"x": 10, "y": 20}}
        over = {"c": {"x": 99}}
        result = deep_merge(base, over)
        assert result == {"a": 1, "b": 2, "c": {"x": 99, "y": 20}}


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 4: ìŠ¤í† ì–´/ì½”ì–´ ëŒ€ê·œëª¨ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestLargePipelineStress:
    """ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸."""

    @pytest.mark.asyncio
    async def test_index_50_files(self, tmp_path):
        """50ê°œ íŒŒì¼ì„ ì¸ë±ì‹±í•˜ê³  ê²€ìƒ‰."""
        # 50ê°œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±
        for i in range(LARGE_FILE_COUNT):
            content = f"# ë¬¸ì„œ {i}: ì£¼ì œ\n\n"
            content += f"ì´ê²ƒì€ ë¬¸ì„œ ë²ˆí˜¸ {i}ì…ë‹ˆë‹¤. "
            content += f"í‚¤ì›Œë“œ: AI, ë¨¸ì‹ ëŸ¬ë‹, ë²¡í„°ê²€ìƒ‰, ë¬¸ì„œ{i}. "
            content += "Lorem ipsum dolor sit amet. " * 10
            (tmp_path / f"doc_{i:03d}.md").write_text(content)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count = await zs.index()
            assert count == LARGE_FILE_COUNT  # ê° íŒŒì¼ 1 ì²­í¬
            assert store.count() == LARGE_FILE_COUNT

            # ê²€ìƒ‰
            emb._custom_return = [[0.5] * TEST_DIM]
            results = await zs.search("AI ë²¡í„°ê²€ìƒ‰", top_k=10)
            assert len(results) <= 10
            assert all("content" in r for r in results)
            zs.close()

    @pytest.mark.asyncio
    async def test_index_files_with_many_chunks(self, tmp_path):
        """í° íŒŒì¼ì—ì„œ ìˆ˜ë°± ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ëŠ” íŒŒì´í”„ë¼ì¸."""
        big_md = _generate_large_markdown(
            num_headings=30, paragraphs_per_heading=2, words_per_paragraph=50
        )
        (tmp_path / "big.md").write_text(big_md)

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count = await zs.index()
            assert count >= 30  # ìµœì†Œ 30ê°œ ì²­í¬
            # ì„ë² ë”© í˜¸ì¶œì´ ì‹¤ì œë¡œ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸
            assert len(emb._embed_calls) >= 1
            zs.close()

    @pytest.mark.asyncio
    async def test_incremental_index_add_files(self, tmp_path):
        """íŒŒì¼ ì¶”ê°€ í›„ ì¦ë¶„ ì¸ë±ì‹±."""
        (tmp_path / "initial.md").write_text("# Initial\nFirst document.")

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")

            # ì²« ì¸ë±ì‹±
            count1 = await zs.index()
            assert count1 == 1

            # íŒŒì¼ ì¶”ê°€
            (tmp_path / "added.md").write_text("# Added\nSecond document.")

            # ë‘ ë²ˆì§¸ ì¸ë±ì‹± - ìƒˆ íŒŒì¼ë§Œ ì¸ë±ì‹±
            count2 = await zs.index()
            assert count2 == 1  # ìƒˆ íŒŒì¼ë§Œ
            assert store.count() == 2  # ì´ 2ê°œ

            zs.close()

    @pytest.mark.asyncio
    async def test_incremental_index_modify_file(self, tmp_path):
        """íŒŒì¼ ìˆ˜ì • í›„ ì¦ë¶„ ì¸ë±ì‹±ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦."""
        file_path = tmp_path / "changeable.md"
        file_path.write_text("# Version 1\nOriginal content.")

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")

            # ì²« ì¸ë±ì‹±
            await zs.index()
            initial_count = store.count()
            assert initial_count == 1

            # íŒŒì¼ ìˆ˜ì • (ë‚´ìš© ë³€ê²½)
            file_path.write_text("# Version 2\nModified content with new info.")

            # ë‘ ë²ˆì§¸ ì¸ë±ì‹± - ë³€ê²½ëœ ì²­í¬ê°€ ì—…ë°ì´íŠ¸ë¨
            count2 = await zs.index()
            assert count2 == 1  # ìƒˆ ì²­í¬ 1ê°œ
            # ì´ì „ í•´ì‹œê°€ ì‚­ì œë˜ê³  ìƒˆ í•´ì‹œê°€ ì¶”ê°€ë¨
            assert store.count() == 1

            zs.close()

    @pytest.mark.asyncio
    async def test_force_reindex_large_dataset(self, tmp_path):
        """ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ ê°•ì œ ì¬ì¸ë±ì‹±."""
        for i in range(20):
            (tmp_path / f"doc_{i}.md").write_text(f"# Doc {i}\nContent for doc {i}.")

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")

            # ì²« ì¸ë±ì‹±
            count1 = await zs.index()
            assert count1 == 20

            # ê°•ì œ ì¬ì¸ë±ì‹±
            count2 = await zs.index(force=True)
            assert count2 == 20  # ëª¨ë“  ì²­í¬ ì¬ì¸ë±ì‹±
            assert store.count() == 20

            zs.close()

    @pytest.mark.asyncio
    async def test_search_top_k_variations(self, tmp_path):
        """ë‹¤ì–‘í•œ top_k ê°’ìœ¼ë¡œ ê²€ìƒ‰."""
        for i in range(30):
            (tmp_path / f"doc_{i}.md").write_text(f"# Doc {i}\nContent about topic {i}.")

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            await zs.index()

            emb._custom_return = [[0.5] * TEST_DIM]

            for k in [1, 3, 5, 10, 20, 50]:
                results = await zs.search("topic", top_k=k)
                assert len(results) <= k
                assert all(isinstance(r["score"], float) for r in results)

            zs.close()

    @pytest.mark.asyncio
    async def test_unicode_content_pipeline(self, tmp_path):
        """ìœ ë‹ˆì½”ë“œ ì½˜í…ì¸ ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸."""
        (tmp_path / "korean.md").write_text(_generate_unicode_markdown())
        (tmp_path / "complex.md").write_text(_generate_complex_nested_markdown())

        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")
            count = await zs.index()
            assert count >= 10  # ë‘ íŒŒì¼ì—ì„œ ìµœì†Œ 10ê°œ ì²­í¬

            # ìœ ë‹ˆì½”ë“œ ì½˜í…ì¸ ê°€ ì •ìƒ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
            stored_contents = [d["content"] for d in store._docs.values()]
            has_korean = any("í•œêµ­ì–´" in c for c in stored_contents)
            assert has_korean, "í•œêµ­ì–´ ì½˜í…ì¸ ê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ"

            zs.close()

    @pytest.mark.asyncio
    async def test_multiple_index_search_cycles(self, tmp_path):
        """ì¸ë±ì‹±-ê²€ìƒ‰ ì‚¬ì´í´ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ."""
        store = _make_stateful_store()
        emb = _make_embedder()

        with patch("zvecsearch.core.get_provider", side_effect=lambda *a, **kw: emb), \
             patch("zvecsearch.core.ZvecStore", side_effect=lambda *a, **kw: store):
            zs = ZvecSearch(paths=[str(tmp_path)], zvec_path="/tmp/fake")

            for cycle in range(5):
                # ìƒˆ íŒŒì¼ ì¶”ê°€
                (tmp_path / f"cycle_{cycle}.md").write_text(
                    f"# Cycle {cycle}\nContent for cycle {cycle}."
                )

                # ì¸ë±ì‹±
                count = await zs.index()
                assert count == 1  # ë§¤ë²ˆ ìƒˆ íŒŒì¼ 1ê°œë§Œ

                # ê²€ìƒ‰
                emb._custom_return = [[0.5] * TEST_DIM]
                results = await zs.search("cycle", top_k=50)
                assert len(results) == cycle + 1  # ëˆ„ì 

            assert store.count() == 5
            zs.close()


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 5: í•´ì‹œ/ID ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestHashIntegrity:
    """í•´ì‹œì™€ ì²­í¬ IDì˜ ë¬´ê²°ì„± ê²€ì¦."""

    def test_1000_unique_chunk_ids(self):
        """1000ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì…ë ¥ì—ì„œ ê³ ìœ  ì²­í¬ ID ìƒì„±."""
        ids = set()
        for i in range(1000):
            cid = compute_chunk_id(f"file_{i}.md", i, i + 10, f"hash_{i}", "model")
            ids.add(cid)
        assert len(ids) == 1000

    def test_chunk_hash_deterministic_across_calls(self):
        """ë™ì¼ ì…ë ¥ì— ëŒ€í•œ í•´ì‹œ ê²°ì •ì„±."""
        for _ in range(100):
            c1 = Chunk(content="test content", source="a.md", heading="H",
                       heading_level=1, start_line=1, end_line=5)
            c2 = Chunk(content="test content", source="a.md", heading="H",
                       heading_level=1, start_line=1, end_line=5)
            assert c1.content_hash == c2.content_hash

    def test_similar_content_different_hashes(self):
        """ë¯¸ì„¸í•˜ê²Œ ë‹¤ë¥¸ ì½˜í…ì¸ ë„ ë‹¤ë¥¸ í•´ì‹œë¥¼ ê°€ì ¸ì•¼ í•¨."""
        hashes = set()
        for i in range(200):
            # ë§¤ë²ˆ ê³ ìœ í•œ ì½˜í…ì¸  ìƒì„±
            content = f"The quick brown fox jumps over the lazy dog number {i}"
            c = Chunk(content=content, source="a.md", heading="",
                      heading_level=0, start_line=1, end_line=1)
            hashes.add(c.content_hash)
        # ëª¨ë“  ê³ ìœ  ì½˜í…ì¸ ì— ëŒ€í•´ ê³ ìœ  í•´ì‹œ
        assert len(hashes) == 200

    def test_chunk_id_all_params_affect_result(self):
        """chunk_idì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ê°€ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ê²€ì¦."""
        base_id = compute_chunk_id("file.md", 1, 10, "hash1", "model1")

        # ê° ë§¤ê°œë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ë©´ IDê°€ ë‹¬ë¼ì ¸ì•¼ í•¨
        assert compute_chunk_id("other.md", 1, 10, "hash1", "model1") != base_id
        assert compute_chunk_id("file.md", 2, 10, "hash1", "model1") != base_id
        assert compute_chunk_id("file.md", 1, 11, "hash1", "model1") != base_id
        assert compute_chunk_id("file.md", 1, 10, "hash2", "model1") != base_id
        assert compute_chunk_id("file.md", 1, 10, "hash1", "model2") != base_id


# ===========================================================================
# í…ŒìŠ¤íŠ¸ ê·¸ë£¹ 6: CLI ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# ===========================================================================
class TestCLIStress:
    """CLIì˜ ëŒ€ê·œëª¨ ì¶œë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸."""

    def test_config_list_contains_all_sections(self):
        """config listê°€ ëª¨ë“  ì„¤ì • ì„¹ì…˜ì„ í¬í•¨í•˜ëŠ”ì§€ ê²€ì¦."""
        from click.testing import CliRunner
        from zvecsearch.cli import cli

        runner = CliRunner()
        result = runner.invoke(cli, ["config", "list", "--resolved"])
        assert result.exit_code == 0
        for section in ["zvec", "index", "embedding", "compact", "chunking", "watch"]:
            assert section in result.output

    def test_help_for_all_commands(self):
        """ëª¨ë“  CLI ëª…ë ¹ì— ëŒ€í•œ --help ì •ìƒ ë™ì‘."""
        from click.testing import CliRunner
        from zvecsearch.cli import cli

        runner = CliRunner()
        commands = ["index", "search", "watch", "compact", "stats",
                     "reset", "expand", "transcript", "config"]
        for cmd in commands:
            result = runner.invoke(cli, [cmd, "--help"])
            assert result.exit_code == 0, f"'{cmd} --help' ì‹¤íŒ¨: {result.output}"
            assert "Usage" in result.output or "usage" in result.output.lower()
